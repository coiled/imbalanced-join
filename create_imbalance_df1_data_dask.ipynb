{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc51328",
   "metadata": {},
   "source": [
    "# Imbalanced Data Join\n",
    "\n",
    "## Background\n",
    "\n",
    "We've run into some applied situations in the wild where joins on highly imbalanced data behave poorly in Dask. In some situtions the run time is more than desired, and in others workers may die unexpectedly. The purpose of this notebook is to create two simulated data sets that exhibits the type of problematic imbalance that we've encountred in the wild. We'll have a few parameters in here so we can create datasets of different sizes that exhibit the gnarly imbalance and these can be used for practical test cases. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b348bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f21b26e-2733-4ea4-b2ac-93246bbaee88",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "These are constants used below, they are described in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86632d90-7167-4ae1-a376-35b3a710671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 constants:\n",
    "\n",
    "bucket_n =  40_000   \n",
    "key_n    =  7000  \n",
    "group1_n = 10\n",
    "group2_n = 20\n",
    "group3_n = 30\n",
    "group4_n = 40\n",
    "\n",
    "P_maj1 = (160, 125125)     # parameters of the majority exp dist\n",
    "P_min1 = (1006402, 8200726) # parameters of the minority exp dist\n",
    "\n",
    "# df2 constants:\n",
    "\n",
    "df2_groupings_n = 11578  \n",
    "\n",
    "P_maj2 = (1.0, 1.5 )      # parameters for the majority exp dist\n",
    "P_min2 = (101.0, 554.0 )  # parameters for the minority exp dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f02a9-3ab1-4cda-bd64-5cd5a234726b",
   "metadata": {},
   "source": [
    "## `df1` - data with large record count\n",
    "\n",
    "The first dataframe is going to be  `df1` and it is the larger table in terms of record count. The fields in `df1` are as follows:\n",
    "\n",
    "`key`: the key used to join with `df2` later. The number of unique keys is low compared to the number of records in each key\n",
    "\n",
    "`bucket1`: Every key has the same number of unique values in `bucket1`. That number is set by the constant `bucket_n` below\n",
    "\n",
    "`group1`: There are 4 groupings. Each of these groupings is assinged a key and they are later used in a group by \n",
    "\n",
    "`group2`: see above\n",
    "\n",
    "`group3`: see above\n",
    "\n",
    "`group4`: see above\n",
    "\n",
    "`value`: This is the value we will sum later. It's random and has no impact on the process in any way. \n",
    "\n",
    "***side note:*** not a bad idea to allow an arbitrary number of groupings sent by a parameter for testing. May iterate on that later. \n",
    "\n",
    "\n",
    "\n",
    "## `df2` - smaller dataset that puts `df1` into some groupings\n",
    "\n",
    "\n",
    "`df2_grouping`: every key is in at least one, and often many groupings. \n",
    "\n",
    "`key`: Key to join to `df1` - not unique in *either* df1 nor df2. This is a many to many join on highly imbalanced groupings. \n",
    "\n",
    "\n",
    "## Simulate `df1`\n",
    "\n",
    "Let's build `df1` first:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f4f91-1525-42aa-8e63-2c5e8c96ebd6",
   "metadata": {},
   "source": [
    "Using the original problematic data, the number of records per bucket is distributed like a double exponential distribution (i.e. two exponential distributions mixed together). 95% of the buckets get their draw from a somewhat shorter tailed distribution, and 5% get their number of record draws from a much longer tail distribution.\n",
    "\n",
    "This will give us roughly half a million sims per key if we use the exponential parameters set up above (`P_maj1` & `P_min1` ) and 40,000 buckets per key.\n",
    "\n",
    "95% first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de72c91-701d-470f-9338-5603da501de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "draws_per_key_majority = st.expon.rvs(*P_maj1, size= round(.95 * key_n)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e30a7-1a52-45b5-87f9-1a5bc172f3e4",
   "metadata": {},
   "source": [
    "then the 5%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7624d6e7-debc-40d6-b541-5c33233f354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "draws_per_key_minority = st.expon.rvs(*P_min1,  size= round(.05 * key_n)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136cbb80-eb33-4e9a-8c2a-45a6714cc0f8",
   "metadata": {},
   "source": [
    "note that doing fractions from one dist then another fraction from another can end up with an off by one error. We might have 10 keys but only end up simulating 9. The probability of this happening goes down as number of keys goes up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807753f1-3fd9-4da8-b878-fda0c86b950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "draws_per_key = np.concatenate((draws_per_key_majority, draws_per_key_minority))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd09378-ad3c-4a24-88c0-4f3f3346ace1",
   "metadata": {},
   "source": [
    "Total number of records that will be in `df1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a0c2db-7a2e-4610-a4ca-55c05017d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(draws_per_key) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564e98b6-a9de-4767-989b-afc6cd27fc7f",
   "metadata": {},
   "source": [
    "So now we know how many buckets and how many records per bucket. So the simulation of `df1` will be to loop over the `draws_per_bucket` and draw that many observations with random groups. This could all be vectorized but I'm keeping this a loop to keep it readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e41119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df1_files(draws, key):\n",
    "    \n",
    "    bucket_n =  40_000   \n",
    "    group1_n = 10\n",
    "    group2_n = 20\n",
    "    group3_n = 30\n",
    "    group4_n = 40\n",
    "        \n",
    "    df_key = np.resize(key, draws)\n",
    "    \n",
    "    df_bucket = np.resize(np.arange(1, bucket_n + 1), draws)\n",
    "\n",
    "    df_group1 = np.random.randint(low=1, high=group1_n, size=draws)\n",
    "    df_group2 = np.random.randint(low=1, high=group2_n, size=draws)\n",
    "    df_group3 = np.random.randint(low=1, high=group3_n, size=draws)\n",
    "    df_group4 = np.random.randint(low=1, high=group4_n, size=draws)\n",
    "    df_value = np.random.random(size=draws)\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        dict(\n",
    "            zip(\n",
    "                [\"key\", \"bucket\", \"group1\", \"group2\", \"group3\", \"group4\", \"value\"],\n",
    "                [df_key, df_bucket, df_group1, df_group2, df_group3, df_group4, df_value],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df.to_parquet(f's3://test-imbalanced-join/df1/key_{str(key).zfill(5)}.parquet')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344cc690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46853824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coiled import Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf2769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = Cluster(name=\"imbalanced_data\",\n",
    "                  n_workers=50, \n",
    "                  package_sync=True,\n",
    "                  scheduler_options={\"idle_timeout\": \"2 hour\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d31c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c552bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42080417",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = client.map(\n",
    "            lambda draws, key: create_df1_files(draws, key), draws_per_key, range(1, key_n+1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc6edd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
