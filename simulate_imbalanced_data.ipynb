{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f94438-5aaf-473d-8ca2-53b354d43856",
   "metadata": {},
   "source": [
    "# Imbalanced Data Join\n",
    "\n",
    "## Background\n",
    "\n",
    "We've run into some applied situations in the wild where joins on highly imbalanced data behave poorly in Dask. In some situtions the run time is more than desired, and in others workers may die unexpectedly. The purpose of this notebook is to create two simulated data sets that exhibits the type of problematic imbalance that we've encountred in the wild. We'll have a few parameters in here so we can create datasets of different sizes that exhibit the gnarly imbalance and these can be used for practical test cases. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17b714c-584f-45a2-87d1-24787c19e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f21b26e-2733-4ea4-b2ac-93246bbaee88",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "These are constants used below, they are described in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86632d90-7167-4ae1-a376-35b3a710671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 constants:\n",
    "\n",
    "bucket_n =  40_000   \n",
    "key_n    =  7000  \n",
    "group1_n = 10\n",
    "group2_n = 20\n",
    "group3_n = 30\n",
    "group4_n = 40\n",
    "\n",
    "P_maj1 = (160, 125125)     # parameters of the majority exp dist\n",
    "P_min1 = (1006402, 8200726) # parameters of the minority exp dist\n",
    "\n",
    "# df2 constants:\n",
    "\n",
    "df2_groupings_n = 11578  \n",
    "\n",
    "P_maj2 = (1.0, 1.5 )      # parameters for the majority exp dist\n",
    "P_min2 = (101.0, 554.0 )  # parameters for the minority exp dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f02a9-3ab1-4cda-bd64-5cd5a234726b",
   "metadata": {},
   "source": [
    "## `df1` - data with large record count\n",
    "\n",
    "The first dataframe is going to be  `df1` and it is the larger table in terms of record count. The fields in `df1` are as follows:\n",
    "\n",
    "`key`: the key used to join with `df2` later. The number of unique keys is low compared to the number of records in each key\n",
    "\n",
    "`bucket1`: Every key has the same number of unique values in `bucket1`. That number is set by the constant `bucket_n` below\n",
    "\n",
    "`group1`: There are 4 groupings. Each of these groupings is assinged a key and they are later used in a group by \n",
    "\n",
    "`group2`: see above\n",
    "\n",
    "`group3`: see above\n",
    "\n",
    "`group4`: see above\n",
    "\n",
    "`value`: This is the value we will sum later. It's random and has no impact on the process in any way. \n",
    "\n",
    "***side note:*** not a bad idea to allow an arbitrary number of groupings sent by a parameter for testing. May iterate on that later. \n",
    "\n",
    "\n",
    "\n",
    "## `df2` - smaller dataset that puts `df1` into some groupings\n",
    "\n",
    "\n",
    "`df2_grouping`: every key is in at least one, and often many groupings. \n",
    "\n",
    "`key`: Key to join to `df1` - not unique in *either* df1 nor df2. This is a many to many join on highly imbalanced groupings. \n",
    "\n",
    "\n",
    "## Simulate `df1`\n",
    "\n",
    "Let's build `df1` first:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f4f91-1525-42aa-8e63-2c5e8c96ebd6",
   "metadata": {},
   "source": [
    "Using the original problematic data, the number of records per bucket is distributed like a double exponential distribution (i.e. two exponential distributions mixed together). 95% of the buckets get their draw from a somewhat shorter tailed distribution, and 5% get their number of record draws from a much longer tail distribution.\n",
    "\n",
    "This will give us roughly half a million sims per key if we use the exponential parameters set up above (`P_maj1` & `P_min1` ) and 40,000 buckets per key.\n",
    "\n",
    "95% first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de72c91-701d-470f-9338-5603da501de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "draws_per_key_majority = st.expon.rvs(*P_maj1, size= round(.95 * key_n)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e30a7-1a52-45b5-87f9-1a5bc172f3e4",
   "metadata": {},
   "source": [
    "then the 5%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7624d6e7-debc-40d6-b541-5c33233f354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "draws_per_key_minority = st.expon.rvs(*P_min1,  size= round(.05 * key_n)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136cbb80-eb33-4e9a-8c2a-45a6714cc0f8",
   "metadata": {},
   "source": [
    "note that doing fractions from one dist then another fraction from another can end up with an off by one error. We might have 10 keys but only end up simulating 9. The probability of this happening goes down as number of keys goes up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807753f1-3fd9-4da8-b878-fda0c86b950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "draws_per_key = np.concatenate((draws_per_key_majority, draws_per_key_minority))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd09378-ad3c-4a24-88c0-4f3f3346ace1",
   "metadata": {},
   "source": [
    "Total number of records that will be in `df1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a0c2db-7a2e-4610-a4ca-55c05017d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(draws_per_key) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564e98b6-a9de-4767-989b-afc6cd27fc7f",
   "metadata": {},
   "source": [
    "So now we know how many buckets and how many records per bucket. So the simulation of `df1` will be to loop over the `draws_per_bucket` and draw that many observations with random groups. This could all be vectorized but I'm keeping this a loop to keep it readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef865d-32b4-451d-8c73-87b44bfba2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## 33 min on my MBP and generates ~52GB of parquet files\n",
    "\n",
    "df1_list = []\n",
    "key = 1\n",
    "\n",
    "# simulate values for each bucket. Simulate a df with a single value for the key then randomly assign groups and values\n",
    "\n",
    "for draws in draws_per_key:\n",
    "    df = pd.DataFrame()\n",
    " \n",
    "    df[\"key\"] = np.resize(key, draws)\n",
    "    \n",
    "    df[\"bucket\"] = np.resize(np.arange(1, bucket_n + 1), draws)\n",
    "\n",
    "    df[\"group1\"] = np.random.randint(low=1, high=group1_n, size=draws)\n",
    "    df[\"group2\"] = np.random.randint(low=1, high=group2_n, size=draws)\n",
    "    df[\"group3\"] = np.random.randint(low=1, high=group3_n, size=draws)\n",
    "    df[\"group4\"] = np.random.randint(low=1, high=group4_n, size=draws)\n",
    "    df[\"value\"] = np.random.random(size=draws)\n",
    "    \n",
    "    #df1_list.append(df)\n",
    "    df.to_parquet(f'./output/df1/key_{str(key).zfill(5)}.parquet')\n",
    "    \n",
    "    key = key + 1\n",
    "    \n",
    "# bring them all together in a single df\n",
    "# when we get ready to write out the larget than ram version, we can change this to write a parquet file\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed9eb6-aa9a-499b-8ca6-f20941f0f327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cd61a04-d139-4241-952b-b95079caa2a1",
   "metadata": {},
   "source": [
    "At this point we should have a full `df1`. \n",
    "\n",
    "## Simulate `df2`\n",
    "\n",
    "`df2` is the smaller dataframe that will be (many to many) joined to `df1`. \n",
    "\n",
    "let's pull together some constants for `df2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df2fdb3-48b2-41ef-b0f6-96e80749680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#magic distribution params tuned using source data\n",
    "\n",
    "draws_per_df2group_majority = st.expon.rvs(*P_maj2, size= int(.99 * df2_groupings_n)).astype(int)\n",
    "\n",
    "draws_per_df2group_minority = st.expon.rvs(*P_min2, size= int(.01 * df2_groupings_n)).astype(int)\n",
    "\n",
    "draws_per_group = np.concatenate([draws_per_df2group_majority, draws_per_df2group_minority])\n",
    "\n",
    "# maximum draws can't exceed key_n\n",
    "draws_per_group = np.clip(draws_per_group, a_min=0, a_max=key_n)\n",
    "\n",
    "# however we want to make sure at least one group has everything... so we force the last record to be key_n\n",
    "# for stress testing we might make the last 5 or more be everyting \n",
    "draws_per_group[draws_per_group.size - 1] = key_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b57330-3c27-452e-b124-bff1b26c4a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f450e4-88ba-49ee-9664-ddb4f4dc495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2_list = []\n",
    "df2_group = 1\n",
    "\n",
    "# simulate values for each bucket. Simulate a df with a single value for the bucket then randomly assign groups and values\n",
    "for draws in draws_per_group:\n",
    "    df = pd.DataFrame()\n",
    " \n",
    "    df[\"df2_group\"] = np.resize(df2_group, draws)\n",
    "    df[\"key\"] = np.random.choice(a=np.arange(1, key_n+1), size = draws, replace=False) # draw draws number of times from 1:key_n\n",
    "   \n",
    "    \n",
    "    df2_list.append(df)\n",
    "    \n",
    "    df2_group = df2_group + 1\n",
    "    \n",
    "# bring them all together in a single df\n",
    "# df2 should never need partitioning so when writing this out to parquet, do it after the concat\n",
    "df2 = pd.concat(df2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09360a81-f74a-4fc2-b1d1-dab97290cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd688ff-bffa-46c3-9fea-127c1af4fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_parquet(f'./output/df2/df2.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7c9d8-354a-45af-846c-60ec787bb88d",
   "metadata": {},
   "source": [
    "## Desired output\n",
    "\n",
    "The desired operation to make stable is to join `df1` to `df2` by `key` which is a many to many join. Then collapse that by `df2_group`, `bucket`, and the four `group` variables then do a `sum(value)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9538ba2-8134-4aa8-a8b1-2d4ddebdd0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_stuff",
   "language": "python",
   "name": "data_stuff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
